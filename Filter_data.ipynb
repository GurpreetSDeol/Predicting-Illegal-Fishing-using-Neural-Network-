{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(rf'D:\\Datasets\\Illegal Fishing\\Original Data\\Ship data\\*.csv')\n",
    "\n",
    "df_list = []\n",
    "# Loop through each file and add the shiptype column\n",
    "for file in csv_files:\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    shiptype = os.path.basename(file).replace('.csv', '')\n",
    "\n",
    "     # Drop rows where is_fishing = -1\n",
    "    df = df[df['is_fishing'] != -1]\n",
    "\n",
    "    df['shiptype'] = shiptype\n",
    "    df_list.append(df)\n",
    "\n",
    "ship_data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "ship_data['timestamp'] = pd.to_datetime(ship_data['timestamp'], unit='s')\n",
    "if 'Unnamed: 0' in ship_data.columns:\n",
    "    ship_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "\n",
    "ship_data.to_csv(rf'D:\\Datasets\\Illegal Fishing\\Processed Data\\Complete_ship_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File paths\n",
    "gdb_path = rf'D:\\Datasets\\Illegal Fishing\\Original Data\\WDPA_Jun2024_Public.gdb'\n",
    "gdb_path_ocean = rf'D:\\Datasets\\Illegal Fishing\\Original Data\\ne_110m_ocean\\ne_110m_ocean.shp'\n",
    "gdb_path_lakes = rf'D:\\Datasets\\Illegal Fishing\\Original Data\\ne_110m_rivers_lake_centerlines\\ne_110m_rivers_lake_centerlines.shp'\n",
    "\n",
    "# Read the ocean and lakes shapefiles\n",
    "ocean_data = gpd.read_file(gdb_path_ocean)\n",
    "lake_data = gpd.read_file(gdb_path_lakes)\n",
    "\n",
    "# Combine ocean and lakes into one GeoDataFrame (union of the geometries)\n",
    "water_bodies = pd.concat([ocean_data, lake_data])\n",
    "\n",
    "# Read the polygon and point data\n",
    "poly_data = gpd.read_file(gdb_path, layer='WDPA_poly_Jun2024')\n",
    "point_data = gpd.read_file(gdb_path, layer='WDPA_point_Jun2024')\n",
    "\n",
    "# Define paths for filtered outputs\n",
    "poly_filtered_path = rf'D:\\Datasets\\Illegal Fishing\\Processed Data\\Filtered_Poly.shp'\n",
    "point_filtered_path = rf'D:\\Datasets\\Illegal Fishing\\Processed Data\\Filtered_Point.shp'\n",
    "\n",
    "# Manually chunk the poly_data and point_data\n",
    "def process_in_chunks(data, water_bodies, output_path, chunk_size=1000):\n",
    "    total_len = len(data)\n",
    "    for start in range(0, total_len, chunk_size):\n",
    "        end = min(start + chunk_size, total_len)\n",
    "        chunk = data.iloc[start:end]\n",
    "\n",
    "        # Spatial join for each chunk\n",
    "        chunk_within_water = gpd.sjoin(chunk, water_bodies, how='inner', predicate='intersects')\n",
    "\n",
    "        # Save chunk to file (append mode after the first chunk)\n",
    "        chunk_within_water.to_file(output_path, mode='a' if start > 0 else 'w')\n",
    "\n",
    "# Process polygon data in chunks\n",
    "process_in_chunks(poly_data, water_bodies, poly_filtered_path)\n",
    "\n",
    "# Process point data in chunks\n",
    "process_in_chunks(point_data, water_bodies, point_filtered_path)\n",
    "\n",
    "print(\"Filtered polygon and point data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File paths for the original shapefiles\n",
    "poly_file_path = r'D:\\Datasets\\Illegal Fishing\\Processed Data\\Filtered_Poly.shp'\n",
    "point_file_path = r'D:\\Datasets\\Illegal Fishing\\Processed Data\\Filtered_Point.shp'\n",
    "\n",
    "# Load the shapefiles\n",
    "gdf_poly = gpd.read_file(poly_file_path)\n",
    "gdf_point = gpd.read_file(point_file_path)\n",
    "\n",
    "# Select only the 'geometry' and 'status_yr' columns\n",
    "gdf_poly_reduced = gdf_poly[['geometry', 'STATUS_YR']]\n",
    "gdf_point_reduced = gdf_point[['geometry', 'STATUS_YR']]\n",
    "\n",
    "# Ensure the CRS is set to EPSG:4326\n",
    "gdf_poly_reduced = gdf_poly_reduced.to_crs(epsg=4326)\n",
    "gdf_point_reduced = gdf_point_reduced.to_crs(epsg=4326)\n",
    "\n",
    "# Save the edited shapefiles\n",
    "Updated_poly_file_path = r'D:\\Datasets\\Illegal Fishing\\Processed Data\\Filtered MPA data\\Filtered_Poly.shp'\n",
    "Updated_point_file_path = r'D:\\Datasets\\Illegal Fishing\\Processed Data\\Filtered MPA data\\\\Filtered_Point.shp'\n",
    "\n",
    "gdf_poly_reduced.to_file(Updated_poly_file_path)  # Overwrite the original file\n",
    "gdf_point_reduced.to_file(Updated_point_file_path)  # Overwrite the original file\n",
    "\n",
    "print(\"Shapefiles edited and saved successfully.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
